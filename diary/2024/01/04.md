---
title: "年末最後の休暇"
description:
date: 2024-01-04
draft: false
categories:
  - "diary"
tags:
  - "life"
---

# 英語

## [GitHub - DLYuanGod/TinyGPT-V: TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones](https://github.com/DLYuanGod/TinyGPT-V)

毎回 25 分以内におさまらないので、今日はちょうど repository のリンクがあがっていたので、

ここの文章をつかってみようとおもう。

TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones

TinyGPT-V は 小さなバックボーンをもつ効率的なマルチモーダル LLM。

コードと環境の準備として git clone 後に environment.yml を conda でよみこんで環境を生成するみたいだ。

TinyGPT-V は Phi-2 をベースにしているため LLM weights は Phi-2 を利用できる。

事前トレーニングデータリンクが各ステージごとに用意されている。

次に transformer の Phi-2 モデルを更新する。

ここまでの設定をおこなうと、記事内にあるステージごとに動作するデモコードを実行し、

ローカルで起動できる。

より強力なモデルを利用するなら、16bit LLMs をデフォルトでよみこむようにされます。

この設定には GPU memory が だいたい 8G 必要になる。

より GPU メモリをセーブしたいなら、low_resource を True にすることで 8G device 未満の 8bit model で実行できる。

トレーニングではまずはじめに LLM の計算制度の重みの調整が必要になる。

**ステージごとの学習データセットや詳細については個別のレドメにあるので割愛**

このリポジトリのライセンスは BSD 3-Clause License

### メモ

- Efficient 効率的な
- Prepare 準備
- perfer 好む
- relevant 関連する
- precision 精度
- Acknowledgement 了承
- MLLMs マルチ LLMs 様々な用途でつかえる LLM をさすらしい。

---

# mental

悪=-1, 普=0, 良=1

## 基礎表

| 項目       | 評価 |
| ---------- | ---- |
| 気分の変化 | 0    |
| 睡眠質     | 1    |
| 睡眠寝付き | 1    |
| 睡眠目覚め | 1    |
| 活動レベル | 1    |

## 食事

### 食欲表

| 項目 | 評価 |
| ---- | ---- |
| 朝   | 1    |
| 昼   | 1    |
| 夜   |      |

### 朝

ブリとえのきとたまねぎのホイル焼

### 昼

### 夜

## 活動内容

### 思考パターン表

| 項目 | 評価 |
| ---- | ---- |
| 朝   | 0    |
| 昼   | 1    |
| 夜   |      |

## ストレス要因

## 思考パターン

朝はすこし機嫌がわるかったが、許容範囲内
